# -*- coding: utf-8 -*-
"""Final SteepestDescentOptimisation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16_d8bspBrQgGaBw9zQvyChEKMlnl9k-I
"""

import numpy as np
import scipy.optimize as opt
from sympy import *
import math
import matplotlib.pyplot as plt

x = Symbol("x")
y = Symbol("y")
z = Symbol("z")
n = Symbol("n")
Xold = Matrix([0, 0, 0])
Init_Symbols = [Symbol('x'),Symbol('y'),Symbol('z')]
G = 0
GD= 0
Gn = 0
Xsub = Matrix([0, 0, 0])
x_init = 0.0001
counter = 0
Mag_GV = 1
counterarr = np.zeros(300)
MagGVarr = np.zeros(300)
MagFXarr = np.zeros(300)

def FN1(x):
    return 3*x[0]-cos(x[1]*x[2])-0.5
def FN2(x):
    return x[0]**2-81*(x[1]+0.1)**2+sin(x[2])+1.06
def FN3(x):
    return exp(-x[0]*x[1])+20*x[2]+(((10*3.1412)-3)/3)

FD = [FN1(Init_Symbols),FN2(Init_Symbols),FN3(Init_Symbols)]

for i in FD:
    GD = GD + 0.5*i**2

GDx = GD.diff(x)
GDy = GD.diff(y)
GDz = GD.diff(z)

while(Mag_GV >= 0.0001):
    counterarr[counter] = counter
    Gradient_Vector= np.array([[GDx.subs([(x, Xold[0]), (y, Xold[1]), (z, Xold[2])])], [GDy.subs([(x, Xold[0]), (y, Xold[1]), (z, Xold[2])])], [GDz.subs([(x, Xold[0]), (y, Xold[1]), (z, Xold[2])])]])
    GVtype = Gradient_Vector.astype(np.float64)
    Mag_GV = np.linalg.norm(GVtype)
    MagGVarr[counter] = Mag_GV
    #print("GV = ", Gradient_Vector)
    Xold = Xold - n*Gradient_Vector
    Gn = GD.subs([(x, Xold[0]), (y, Xold[1]), (z, Xold[2])])
    #print("Gn = ", Gn)
    #print("Xold= ", Xold)
    Gndx = Gn.diff(n)
    Gndxx= Gndx.diff(n)
    for i in range(100):
        x_init = x_init - Gndx.subs(n,x_init)/Gndxx.subs(n,x_init)
    #print("X_init = ", x_init)
    Xold = Xsub - x_init*Gradient_Vector
    Xsub = Xold
    #print("New Point = ", Xold)
    counter+=1

print("Number of Iterations: ", counter)
print("New Point = ", Xold)
plt.plot(counterarr, MagGVarr)
plt.title('Magnitude of Gradient Vector Vs Index Number')
plt.ylabel('Gradient Vector Magnitude')
plt.xlabel('Iteration Number')
plt.show()